{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multi-Armed Bandit Problem\n",
    "\n",
    "## Description\n",
    "\n",
    "The Multi-Armed Bandit Problem (MAB, or often called K or N-armed bandit problems) is a problem where a fixed set of limied resources must be allocated between competing choices in a way that maximizes their expected gain, when the underlying rewards is not known at the start of learning.  This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilema.  The crucial tradeoff the algorithm faces at each trial is between \"exploitation\" of the arm that has the highest expected payoff and \"exploration\" to get more information about the expected payoffs of the other arms. The trade-off between exploration and exploitation is also faced in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics\n",
    "\n",
    "### State Space\n",
    "\n",
    "The state space is represented as $X = [K]^T$ where $K$ is the number of arms and $T$ is the number of timesteps.  Each component represents the number of times the arm has been pulled up to the current iteration.\n",
    "\n",
    "\n",
    "### Action space\n",
    "\n",
    "The action space is $[K]$ representing the index of the arm selected at that time instant.\n",
    "\n",
    "### Reward\n",
    "\n",
    "The reward is calculated via $r(x,a)$ taken as a random sample from a specified distribution $\\mu(a)$.\n",
    "\n",
    "### Transitions\n",
    "\n",
    "From state $x$ having taking action $a$ the agent transitions to a new state $x'$ where $x'[a]$ is incremented by one to denote the increment that the arm $a$ has been selected an extra time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "### Line\n",
    "\n",
    "`reset`\n",
    "\n",
    "Returns the environment to its original state.\n",
    "\n",
    "`step(action)`\n",
    "\n",
    "Takes an action from the agent and returns the state of the system after the next arrival.\n",
    "* `action`: the index of the selected arm\n",
    "\n",
    "Returns:\n",
    "\n",
    "* `state`: The number of times each arm has been selected so far\n",
    "\n",
    "* `reward`: The reward drawn from the distribution specified by the given action.\n",
    "\n",
    "* `pContinue`:\n",
    "\n",
    "* `info`: Empty\n",
    "\n",
    "`render`\n",
    "\n",
    "Currently unimplemented\n",
    "\n",
    "`close`\n",
    "\n",
    "Currently unimplemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic Agents\n",
    "\n",
    "We currently have no heuristic algorithms implemented for this environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
