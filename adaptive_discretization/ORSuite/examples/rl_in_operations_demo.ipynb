{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tender-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-unknown",
   "metadata": {},
   "source": [
    "# OR Suite "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-income",
   "metadata": {},
   "source": [
    "##  Ambulance Routing Environment\n",
    "\n",
    "One potential application of reinforcement learning involves positioning a server or servers (in this case an ambulance) in an optimal way geographically to respond to incoming calls while minimizing the distance traveled by the servers. \n",
    "\n",
    "This is closely related to the [k-server problem](https://en.wikipedia.org/wiki/K-server_problem), where there are $k$ servers stationed in a space that must respond to requests arriving in that space in such a way as to minimize the total distance traveled. \n",
    "\n",
    "The ambulance routing problem addresses the problem by modeling an environment where there are ambulances stationed at locations, and calls come in that one of the ambulances must be sent to respond to. The goal of the agent is to minimize both the distance traveled by the ambulances between calls and the distance traveled to respond to a call by optimally choosing the locations to station the ambulances. The ambulance environment has been implemented in two different ways; as a 1-dimensional number line $[0,1]$ along which ambulances will be stationed and calls will arrive, and a graph with nodes where ambulances can be stationed and calls can arrive, and edges between the nodes that ambulances travel along.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3442d1de",
   "metadata": {},
   "source": [
    "## Line\n",
    "\n",
    "ambulance_metric.py is a 1-dimensional reinforcement learning environment in the space $X = [0, 1]$. Each ambulance in the problem can be located anywhere in $X$, so the state space is $S = X^k$, where $k$ is the number of ambulances. The distance function is chosen by the user, who specifies what kind of norm to use. Calls for an ambulance can also arrive anywhere in $X$, and the nearest ambulance will respond to the call, leaving the locations of the other ambulances unchanged. Between calls the agent must choose a location to station each ambulance, with the goal of minimizing both the distance traveled between calls and to respond to a call.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-carry",
   "metadata": {},
   "source": [
    "### Environment Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "authorized-chester",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from pyvirtualdisplay import Display\n",
    "#display = Display(visible=0, size=(500, 800))\n",
    "#display.start()\n",
    "\n",
    "import or_suite\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "import rendering\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "a = 5\n",
    "b = 2\n",
    "CONFIG = {'epLen': 5,\n",
    "    'arrival_dist': lambda x : np.random.beta(a,b), \n",
    "    'alpha': 0.25, \n",
    "    'starting_state': np.array([0.0, 0.0]), \n",
    "    'num_ambulance': 2,\n",
    "    'norm': 1\n",
    "}\n",
    "\n",
    "alpha = CONFIG['alpha']\n",
    "epLen = CONFIG['epLen']\n",
    "state = CONFIG['starting_state']\n",
    "num_ambulance = CONFIG['num_ambulance']\n",
    "\n",
    "agent = or_suite.agents.ambulance.command_line_metric.commandLineAgent(epLen)\n",
    "env = gym.make('Ambulance-v0', config=CONFIG)\n",
    "env.reset()\n",
    "\n",
    "done = False\n",
    "your_rewards = []\n",
    "heuristic_agent_rewards = []\n",
    "your_total_reward = 0\n",
    "heuristic_agent_total_reward = 0\n",
    "\n",
    "median_est = (a - 1/3)/(a + b - 2/3)\n",
    "heuristic_agent_states = [state]\n",
    "\n",
    "x_axis = ['Your Reward So Far', 'RL Algorithm Reward So Far']\n",
    "\n",
    "\n",
    "def display_animation(screen, time_to_display):\n",
    "    plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    if time_to_display is not None:\n",
    "        ipythondisplay.display(plt.gcf())\n",
    "        time.sleep(time_to_display)\n",
    "\n",
    "def plot_rewards():\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    y_axis = [your_total_reward, heuristic_agent_total_reward]\n",
    "    ax.bar(x_axis, y_axis)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "while not done:\n",
    "    action = agent.greedy(state, 0)\n",
    "    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    your_rewards.append(reward)\n",
    "    your_total_reward += reward\n",
    "\n",
    "    # by comparing the final state with the action the user chose, we can figure out where the most recent arrival was\n",
    "    previous_arrival_ind = np.argmax(np.abs(action - state))\n",
    "    previous_arrival = state[previous_arrival_ind]\n",
    "\n",
    "    # the heuristic agent always chooses to put all the ambulances at the median estimate\n",
    "    heuristic_agent_action = np.full(num_ambulance, median_est)\n",
    "\n",
    "    # the state will have one ambulance where the call arrived, and all other ambulances at the median estimate\n",
    "    # doesn't matter which ambulance responds to the call because they're all at the same place\n",
    "    heuristic_agent_state = np.concatenate([np.full(num_ambulance - 1, median_est), [previous_arrival]])\n",
    "    heuristic_agent_states.append(heuristic_agent_state)\n",
    "\n",
    "    heuristic_agent_reward = -1 * (alpha * np.sum(np.abs(heuristic_agent_states[-2] - heuristic_agent_action)) + (1 - alpha) * np.sum(np.abs(heuristic_agent_action - heuristic_agent_state)))\n",
    "    heuristic_agent_rewards.append(heuristic_agent_reward)\n",
    "    heuristic_agent_total_reward += heuristic_agent_reward\n",
    "    \n",
    "    env.viewer = rendering.PygletWindow(850, 550)\n",
    "    env.viewer.window.set_visible(False)\n",
    "    screen1, screen2, screen3 = env.render(mode='rgb_array')\n",
    "    \n",
    "    # display each step of the environment for 2 seconds\n",
    "    display_animation(screen1, 2)\n",
    "    display_animation(screen2, 2)\n",
    "    display_animation(screen3, None)\n",
    "\n",
    "    # plot your reward vs the agent's reward\n",
    "    plot_rewards()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(\"\\nThe most recent call arrival was at \" + str(previous_arrival) + \", and ambulance \" + str(previous_arrival_ind+1) + \" responded to the call.\\n\")\n",
    "\n",
    "    time.sleep(2)\n",
    "    \n",
    "    if not done:\n",
    "        cont = input(\"Continue? [y/n]\")\n",
    "        if cont == \"n\":\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()\n",
    "\n",
    "if np.sum(your_rewards) > np.sum(heuristic_agent_rewards):\n",
    "    print(\"CONGRATS! You beat the RL algorithm.\")\n",
    "else:\n",
    "    print(\"You did not get a better reward than the RL algorithm.\")\n",
    "\n",
    "print(\"\\nYour total reward over all iterations was \", round(sum(your_rewards),3))\n",
    "print(\"The RL algorithm's total reward over all iterations was \", round(sum(heuristic_agent_rewards),3), \"\\n\")\n",
    "\n",
    "plot_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-aruba",
   "metadata": {},
   "source": [
    "This problem is well-studied in the setting where the number of ambulances $k$ is small and in simple metric spaces.  However,\n",
    "\n",
    "- When the number of ambulances $k$ increases the complexity of the optimal policy increases dramatically\n",
    "- In real-world situations, the problem gets complicated in more 'realistic' metrics (and a dataset based in Ithaca is provided)\n",
    "- People construct a weighted metric for algorithm design, one cares about evaluating and balancing between multiple metrics\n",
    "- How to evaluate if your RL algorithm is good? Need to benchmark against well-known heuristics for this problem to understand the value that RL approach brings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3bba4",
   "metadata": {},
   "source": [
    "## This Code Demo\n",
    "\n",
    "In this code demonstration you will be building off on the implementation of UCBVI and Q Learning from yesterday to implement a model free and model based algorithm with adaptive discretization.  There are two main components which need to be implemented:\n",
    "\n",
    "- Algorithm implementation (modifying the update rules)\n",
    "- Tree based discretization architecture (implementing the \"selection\" rule)\n",
    "\n",
    "In the ```agents.rl.ada_mb``` and ```agents.rl.ada_mb``` file we have the outline of the adaptive discretization algorithms with key components of the code removed and highlighted with TODO.  The tree based discretization is included in the ```utils``` folder.\n",
    "\n",
    "After filling in and updating the required code, the experiments can be run with the included python file.  Note that this file follows the same experiment procedure that was outlined in the jupyter notebook from yesterday, but here we just include a written one to better focus on the agent design."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
