{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tender-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-unknown",
   "metadata": {},
   "source": [
    "Key references:\n",
    "\n",
    "[Sutton and Barto](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "\n",
    "[Windy Grid World](https://github.com/ibrahim-elshar/gym-windy-gridworlds)\n",
    "\n",
    "[Q Learning](https://arxiv.org/abs/1807.03765)\n",
    "\n",
    "[UCBVI](https://proceedings.mlr.press/v70/azar17a.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-income",
   "metadata": {},
   "source": [
    "##  Introduction: Online Tabular Algorithms\n",
    "\n",
    "In the second code demo we will work on implementing ```Q-Learning``` and ```UCBVI``` for tabular OpenAI Gym environments and incorporate them into the ```ORSuite``` package.  We will then use the same package to run simulations on the ```Windy Grid World``` and ```Ambulance routing``` environment, comparing the performance between the two algorithms to randomized performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c691b",
   "metadata": {},
   "source": [
    "## Windy Grid World\n",
    "\n",
    "Windy Grid World is a modification of the standard Grid World task.  Here, there is a fixed grid of possible states, and dedicated start and goal states.  The algorithm starts in the start state, and has four actions (left, right, up, down) and needs to learn where the goal state is through interacting with the environment.  There is one kick: there is a crosswind running upward through the middle of the grid, so in the middle region the resultant next states are shifted upward by ```wind``` whose strength varies either stochastically or deterministically in the different regions.  Here we will be using either:\n",
    "- ```StochWindyGridWorld-v0```\n",
    "as our environments to test out the learning algorithms.\n",
    "\n",
    "Note that the code for these environments is taken and modified from [here](https://github.com/ibrahim-elshar/gym-windy-gridworlds).  I also am not \"registering\" it as part of the package for convenience just to keep the environment code out separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb2f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stoch_windy_gridworld_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e4342be",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = stoch_windy_gridworld_env.StochWindyGridWorldEnv(GRID_HEIGHT=4,  \\\n",
    "                                            GRID_WIDTH=4, \\\n",
    "                                            PROB = [.1, .3, .2, .1], \\\n",
    "                                            START_CELL = (3,0), \\\n",
    "                                            GOAL_CELL = (3,3), \\\n",
    "                                            REWARD=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1132ca52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.contains((0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "297b79a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2baaa36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 0), 0, False, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e72ee5",
   "metadata": {},
   "source": [
    "## Ambulance Routing\n",
    "\n",
    "See ```or_suite.envs.ambulance.ambulance_routing_readme.ipynb``` for a description.\n",
    "\n",
    "One potential application of reinforcement learning involves positioning a server or servers (in this case an ambulance) in an optimal way geographically to respond to incoming calls while minimizing the distance traveled by the servers. This is closely related to the [k-server problem](https://en.wikipedia.org/wiki/K-server_problem), where there are $k$ servers stationed in a space that must respond to requests arriving in that space in such a way as to minimize the total distance traveled. \n",
    "\n",
    "The ambulance routing problem addresses the problem by modeling an environment where there are ambulances stationed at locations, and calls come in that one of the ambulances must be sent to respond to. The goal of the agent is to minimize both the distance traveled by the ambulances between calls and the distance traveled to respond to a call by optimally choosing the locations to station the ambulances. The ambulance environment has been implemented in two different ways; as a 1-dimensional number line $[0,1]$ along which ambulances will be stationed and calls will arrive, and a graph with nodes where ambulances can be stationed and calls can arrive, and edges between the nodes that ambulances travel along.\n",
    "\n",
    "`ambulance_graph.py` is structured as a graph of nodes $V$ with edges between the nodes $E$. Each node represents a location where an ambulance could be stationed or a call could come in. The edges between nodes are undirected and have a weight representing the distance between those two nodes.\n",
    "\n",
    "The nearest ambulance to a call is determined by computing the shortest path from each ambulance to the call, and choosing the ambulance with the minimum length path. The calls arrive using a prespecified iid probability distribution. The default is for the probability of call arrivals to be evenly distributed over all the nodes; however, the user can also choose different probabilities for each of the nodes that a call will arrive at that node. For example, in the following graph the default setting would be for each call to have a 0.25 probability of arriving at each node, but the user could instead specify that there is a 0.1 probability of a call at node 0, and a 0.3 probability of a call arriving at each of the other three nodes.\n",
    "\n",
    "<div>\n",
    "<img src=\"attachment:graph.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "After each call comes in, the agent will choose where to move each ambulance in the graph. Every ambulance except the ambulance that moved to respond to the call will be at the same location where the agent moved it to on the previous iteration, and the ambulance that moved to respond to the call will be at the node where the call came in. \n",
    "\n",
    "The graph environment is currently implemented using the [networkx package](https://networkx.org/documentation/stable/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2188e51",
   "metadata": {},
   "source": [
    "## What makes an algorithm?\n",
    "\n",
    "As discussed during the backgrounds on MDPs earlier today, an algorithm is characterized by a couple main components:\n",
    "- Policy (i.e. how it decides to take actions from a given state)\n",
    "- Update Step (i.e. how it updates its policy based on observed values)\n",
    "\n",
    "In ```orsuite.or_suite.agents.agent.py``` we have an API framework of how an algorithm should work, characterized by these methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ebebe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAll agents should inherit from the Agent class.\\n\\n\\nclass Agent(object):\\n\\n    def __init__(self):\\n        pass\\n\\n    def reset(self):\\n        pass\\n\\n    def update_config(self, env, config):\\n         Update agent information based on the config__file\\n        self.config = config\\n        return\\n        \\n    def update_parameters(self, param):\\n        return\\n\\n    def update_obs(self, obs, action, reward, newObs, timestep, info):\\n        Add observation to records\\n\\n    def update_policy(self, h):\\n        Update internal policy based upon records\\n\\n    def pick_action(self, obs, h):\\n        Select an action based upon the observation\\n\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "All agents should inherit from the Agent class.\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def update_config(self, env, config):\n",
    "         Update agent information based on the config__file\n",
    "        self.config = config\n",
    "        return\n",
    "        \n",
    "    def update_parameters(self, param):\n",
    "        return\n",
    "\n",
    "    def update_obs(self, obs, action, reward, newObs, timestep, info):\n",
    "        Add observation to records\n",
    "\n",
    "    def update_policy(self, h):\n",
    "        Update internal policy based upon records\n",
    "\n",
    "    def pick_action(self, obs, h):\n",
    "        Select an action based upon the observation\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14813e56",
   "metadata": {},
   "source": [
    "In this framework you will notice a couple key components.\n",
    "\n",
    "- reset (resets the agent to ```forget``` what it has learned between experiments)\n",
    "- update config (potentially updates internals of the algorithm based on the config of the environment)\n",
    "- update parameters (potentially updates parameters, e.g. bonus confidence term) for hyperparameter tuning\n",
    "- update obs (updates internal estimates based on one step reward)\n",
    "- update policy (updates the internal policy based on records)\n",
    "- pick action (actually picks the action based on the current state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdbed50",
   "metadata": {},
   "source": [
    "## Randomized Algorithm\n",
    "\n",
    "For example, if we wanted to implement a random algorithm (included in ```or_suite.agents.rl.random.py``` we would simply do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "743d28db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class randomAgent():\n",
    "    \"\"\"Randomized RL Algorithm\n",
    "\n",
    "    Implements the randomized RL algorithm - selection an action uniformly at random from the action space.  In particular,\n",
    "    the algorithm stores an internal copy of the environment's action space and samples uniformly at random from it.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def update_config(self, env, config = None):\n",
    "        \"\"\"Updates configuration file for the agent\n",
    "\n",
    "        Updates the stored environment to sample uniformly from.\n",
    "\n",
    "        Args:\n",
    "            env: an openAI gym environment\n",
    "            config: an (optional) dictionary containing parameters for the environment\n",
    "        \"\"\"\n",
    "\n",
    "        self.environment = env\n",
    "        pass\n",
    "\n",
    "    def update_obs(self, obs, action, reward, newObs, timestep, info):\n",
    "        pass\n",
    "\n",
    "    def update_policy(self, h):\n",
    "        pass\n",
    "\n",
    "    def pick_action(self, obs, h):\n",
    "        \"\"\"Selects an action for the algorithm.\n",
    "\n",
    "        Args:\n",
    "            obs: a state for the environment\n",
    "            h: timestep\n",
    "\n",
    "        Returns:\n",
    "            An action sampled uniformly at random from the environment's action space.\n",
    "        \"\"\"\n",
    "        return self.environment.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e3f08",
   "metadata": {},
   "source": [
    "Note that the only real component is ```pick_action``` which simply picks an action from the environment's defined action space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598e61d",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Your goal for this code demo is to implement [Q Learning](https://arxiv.org/abs/1807.03765)\n",
    "and \n",
    "[UCBVI](https://proceedings.mlr.press/v70/azar17a.html)\n",
    " and run an experiment.  A starter for the code is located in ```or_suite.agents.rl.discrete_mb``` and ```or_suite.agents.rl.discrete_mf```.\n",
    "(Note that solutions are also located in a sub-folder just for checking work).  The high level architecture of the code, and an outline on what to implement, is included in the python files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c029e",
   "metadata": {},
   "source": [
    "## Running an experiment\n",
    "\n",
    "Now that we have the algorithms up and running, our next step is to actually run an experiment to compare the algorithm's performance over time!  We will be using the ```ORSuite``` package (see [here](https://orsuite.readthedocs.io/en/latest/experiment_file.html) for documentation on running experiments) and to generate plots as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ac5a1",
   "metadata": {},
   "source": [
    "### Package Installation\n",
    "\n",
    "First we import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e20a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.dqn import MlpPolicy as MlpPolicy_dqn\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb3c730",
   "metadata": {},
   "source": [
    "### Configure Environment and Pick Problem Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ba8bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "epLen = 5\n",
    "nEps = 5000\n",
    "numIters = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d871ab33",
   "metadata": {},
   "source": [
    "### Simulation Parameters\n",
    "\n",
    "Next we need to specify parameters for the simulation. This includes setting a seed, the frequency to record the metrics, directory path for saving the data files, a deBug mode which prints the trajectory, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929e2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/rideshare/', \n",
    "                    'deBug': False, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': True, \n",
    "                    'epLen' : epLen,\n",
    "                    'render': False,\n",
    "                    'pickle': False\n",
    "                    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c7bf9",
   "metadata": {},
   "source": [
    "### List of Algorithms\n",
    "\n",
    "Next we will pick a list of algorithms to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ef0e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = stoch_windy_gridworld_env.StochWindyGridWorldEnv(GRID_HEIGHT=4,  \\\n",
    "                                            GRID_WIDTH=4, \\\n",
    "                                            START_CELL = (3,0), \\\n",
    "                                            GOAL_CELL = (3,3), \\\n",
    "                                            REWARD = 1, \\\n",
    "                                            PROB = [.1, .3, .2, .1], \\\n",
    "                                            EPLEN=epLen)\n",
    "\n",
    "scaling_list = [1]\n",
    "mon_env = Monitor(env)\n",
    "conf_scale = .5\n",
    "action_space = env.action_space\n",
    "state_space = env.observation_space\n",
    "\n",
    "agents = {\n",
    "    'random': or_suite.agents.rl.random.randomAgent(),\n",
    "#     'UCBVI': or_suite.agents.rl.discrete_mb.DiscreteMB(action_space, state_space, epLen, conf_scale, 0, False),\n",
    "    'QLearning': or_suite.agents.rl.discrete_ql.DiscreteQl(action_space, state_space, epLen, conf_scale)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db1c4b",
   "metadata": {},
   "source": [
    "### Running an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65d8f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random\n",
      "Writing to file data.csv\n",
      "QLearning\n",
      "Tuning parameter: 1\n",
      "Chosen parameters: 1\n",
      "Writing to file data.csv\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "\n",
    "\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    \n",
    "    \n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/grid_world_'+str(agent)+'/'\n",
    "    if agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    elif agent == 'UCBVI' or agent == 'QLearning':\n",
    "        or_suite.utils.run_single_algo_tune(env,agents[agent], scaling_list, DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/grid_world_'+str(agent))\n",
    "    algo_list_line.append(str(agent))\n",
    "\n",
    "\n",
    "fig_path = '../figures/'\n",
    "fig_name = 'grid_world'+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f72ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"500\"\n",
       "            src=\"../figures/grid_world_line_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f2dac094160>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"../figures/grid_world_line_plot.pdf\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342ac8b",
   "metadata": {},
   "source": [
    "### What gives?\n",
    "\n",
    "Well - we have spotted one of the largest pitfalls of online algorithms, exploration in goal based settings.  Typically \"goal based\" MDPs are thought to be the worst case problem instances as the rewards are spare and take a while to propagate through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e602b",
   "metadata": {},
   "source": [
    "### Ambulance Environment\n",
    "\n",
    "Next we will run the experiments on the ambulance graph environment.  In this domain the rewards are not sparse and so we expect to witness learning in just a few number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1995f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space size: [5 5]\n",
      "Action space size: [5 5]\n"
     ]
    }
   ],
   "source": [
    "# Getting out configuration parameter for the environment\n",
    "CONFIG =  or_suite.envs.env_configs.ambulance_graph_default_config\n",
    "\n",
    "\n",
    "# Specifying training iteration, epLen, number of episodes, and number of iterations\n",
    "epLen = CONFIG['epLen']\n",
    "nEps = 1000\n",
    "numIters = 20\n",
    "\n",
    "\n",
    "scaling_list = [0.01, .1, 1, 10]\n",
    "\n",
    "\n",
    "# Configuration parameters for running the experiment\n",
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/ambulance/', \n",
    "                    'deBug': False, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': True, # save trajectory for calculating additional metrics\n",
    "                    'epLen' : 5,\n",
    "                    'render': False,\n",
    "                    'pickle': False # indicator for pickling final information\n",
    "                    }\n",
    "\n",
    "\n",
    "alpha = CONFIG['alpha']\n",
    "num_ambulance = CONFIG['num_ambulance']\n",
    "\n",
    "ambulance_env = gym.make('Ambulance-v1', config=CONFIG)\n",
    "mon_env = Monitor(ambulance_env)\n",
    "\n",
    "state_space = ambulance_env.observation_space\n",
    "action_space = ambulance_env.action_space\n",
    "\n",
    "\n",
    "agents = {\n",
    "'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "'Stable': or_suite.agents.ambulance.stable.stableAgent(CONFIG['epLen']),\n",
    "'UCBVI': or_suite.agents.rl.discrete_mb.DiscreteMB(action_space, state_space, epLen, scaling_list[0], 0, False),\n",
    "'QLearning' : or_suite.agents.rl.discrete_ql.DiscreteQl(action_space, state_space, epLen, scaling_list[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "049ef158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random\n",
      "Writing to file data.csv\n",
      "Stable\n",
      "Writing to file data.csv\n",
      "UCBVI\n",
      "Writing to file data.csv\n",
      "QLearning\n",
      "Writing to file data.csv\n",
      "   Algorithm  Reward      Time   Space\n",
      "0     Random -9.8875  7.268000 -4275.0\n",
      "1     Stable -4.8750  7.537671 -3659.0\n",
      "2      UCBVI -8.3875  4.536713 -4995.4\n",
      "3  QLearning -6.8375  6.548457 -4954.2\n"
     ]
    }
   ],
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/ambulance_metric_'+str(agent)+'_'+str(num_ambulance)+'_'+str(alpha)+'/'\n",
    "    if agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    elif agent == 'DiscreteQL':\n",
    "        or_suite.utils.run_single_algo_tune(ambulance_env,agents[agent], scaling_list, DEFAULT_SETTINGS)\n",
    "    elif agent == 'DiscreteQL':\n",
    "        or_suite.utils.run_single_algo_tune(ambulance_env,agents[agent], scaling_list, DEFAULT_SETTINGS)\n",
    "    elif agent == 'AdaQL' or agent == 'Unif QL' or agent == 'AdaMB' or agent == 'Unif MB'or agent == 'DiscreteMB':\n",
    "        or_suite.utils.run_single_algo_tune(ambulance_env, agents[agent], scaling_list, DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(ambulance_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/ambulance_metric_'+str(agent)+'_'+str(num_ambulance)+'_'+str(alpha))\n",
    "    algo_list_line.append(str(agent))\n",
    "    if agent != 'SB PPO':\n",
    "        path_list_radar.append('../data/ambulance_metric_'+str(agent)+'_'+str(num_ambulance)+'_'+str(alpha))\n",
    "        algo_list_radar.append(str(agent))\n",
    "fig_path = '../figures/'\n",
    "fig_name = 'ambulance_metric'+'_'+str(num_ambulance)+'_'+str(alpha)+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)\n",
    "\n",
    "additional_metric = {}\n",
    "fig_name = 'ambulance_metric'+'_'+str(num_ambulance)+'_'+str(alpha)+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "055b797a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"500\"\n",
       "            src=\"../figures/ambulance_metric_2_0.25_line_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f2cf6e756a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"../figures/ambulance_metric_2_0.25_line_plot.pdf\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f740727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"500\"\n",
       "            src=\"../figures/ambulance_metric_2_0.25_radar_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f2cf6e75af0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"../figures/ambulance_metric_2_0.25_radar_plot.pdf\", width=600, height=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
