{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tender-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-unknown",
   "metadata": {},
   "source": [
    "# OR Suite \n",
    "\n",
    "Key references:\n",
    "\n",
    "[OpenAI Gym Documentation](https://www.gymlibrary.ml/)\n",
    "\n",
    "[Spaces Documentation](https://www.gymlibrary.ml/content/spaces/)\n",
    "\n",
    "[ORSuite Contribution Guide](https://github.com/cornell-orie/ORSuite/blob/main/ORSuite_Contribution_Guide.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-income",
   "metadata": {},
   "source": [
    "##  Introduction: Creating a Custom Simulator\n",
    "\n",
    "In the first code demonstration we will work on creating a ``custom simulator`` using the OpenAI Gym API framework. \n",
    "\n",
    "As we had discussed, the OpenAI Gym comes loaded with a lot of different simulators, ranging from classic control tasks (cartpole, mountaincar, etc) to Atari games.  However, the package still provides an API and outline for creating custom environments which are not part of the Gym package.  Thankfully, the framework is well documented and outlines all of the required steps in order to create your own custom environment.\n",
    "\n",
    "We will additionally incorporate the custom environment into the ORSuite package, an open-source packaged aimed at providing simulator for researchers in RL and Operations to test algorithms on common tasks in operations management.  This is developed by a team of undergraduate students at Cornell, and is slowly building up to contain simulators such as:\n",
    "- inventory control\n",
    "- ridesharing systems\n",
    "- revenue management problems\n",
    "- resource allocation\n",
    "- vaccine allocation\n",
    "and many more.  Maybe some of the simulators that we create today could be incorporated into the package as well!\n",
    "\n",
    "Due to intracacies in developing the code demonstration, you will notice that the version of ``ORSuite`` contained here does not include all of the main components.  This was done so that we can isolate the key components of developing the code demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2188e51",
   "metadata": {},
   "source": [
    "## What makes a simulator?\n",
    "\n",
    "As discussed during the backgrounds on MDPs earlier today, an environment or simulator is specified by the following main components:\n",
    "- action space\n",
    "- state space (called observation space in the Open AI gym API)\n",
    "- starting state distribution\n",
    "- reward function\n",
    "- transition kernel\n",
    "- time horizon\n",
    "\n",
    "The OpenAI Gym API provides an abstraction for each of these.  In essence, our goal will be to create a ``subclass`` of the Environment object created by OpenAI Gym.  The high level sketch of the code will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ebebe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport gym\\nfrom gym import spaces\\n\\nclass CustomEnv(gym.Env):\\n    \"\"\"Custom Environment that follows gym interface\"\"\"\\n    metadata = {\\'render.modes\\': [\\'human\\']}\\n\\n    def __init__(self, arg1, arg2, ...):\\n        super(CustomEnv, self).__init__()    # Define action and observation space\\n        # They must be gym.spaces objects    # Example when using discrete actions:\\n        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)    # Example for using image as input:\\n        self.observation_space = spaces.Box(low=0, high=255, shape=\\n                        (HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\\n\\n    def step(self, action):\\n    # Execute one time step within the environment\\n        ...  \\n    def reset(self):\\n    # Reset the state of the environment to an initial state\\n        ...  \\n    \\n    def render(self, mode=\\'human\\', close=False):\\n    # Render the environment to the screen\\n        ...\\n        \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, arg1, arg2, ...):\n",
    "        super(CustomEnv, self).__init__()    # Define action and observation space\n",
    "        # They must be gym.spaces objects    # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)    # Example for using image as input:\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=\n",
    "                        (HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "    # Execute one time step within the environment\n",
    "        ...  \n",
    "    def reset(self):\n",
    "    # Reset the state of the environment to an initial state\n",
    "        ...  \n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "    # Render the environment to the screen\n",
    "        ...\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14813e56",
   "metadata": {},
   "source": [
    "In this framework you will notice a couple key components.\n",
    "\n",
    "At the top we have the import statements:\n",
    "\n",
    "```\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "```\n",
    "\n",
    "gym is the name of the OpenAI Gym package, and spaces is the part of the package that allows the user to specify the action and observation spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc9b74a",
   "metadata": {},
   "source": [
    "```\n",
    "    def __init__(self, arg1, arg2, ...):\n",
    "        super(CustomEnv, self).__init__()    # Define action and observation space\n",
    "        # They must be gym.spaces objects    # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)    # Example for using image as input:\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=\n",
    "                        (HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
    "\n",
    "```\n",
    "\n",
    "In this statement we initialize the simulator. Note that we are able to pass arguments (i.e. specify the number of ambulances in an ambulance model, price distributions, etc).  Next we specialize the action space and observation space.  These must be ```gym.spaces``` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953dca58",
   "metadata": {},
   "source": [
    "## Gym Spaces\n",
    "\n",
    "Spaces is a superclass that is used to define observation and action spaces, and are crucially used in Gym to define the format of valid actions and states.  They serve a couple of different purposes in running an experiment:\n",
    "- They define how to interact with the environment, i.e. specify what are valid actions\n",
    "- Format data in structured ways to feed into RL algortihms (i.e. numpy vectors)\n",
    "- Provide method to sample elements randomly (used for exploration, debugging, $\\epsilon$ greedy, etc)\n",
    "\n",
    "There are a couple of different formats of spaces, which includes:\n",
    "\n",
    "- ```box```: an n-dimensional continuous feature space with an upper and lower bound for each dimension\n",
    "\n",
    "- ```dict```: a dictionary of simpler spaces and labels for those spaces\n",
    "\n",
    "- ```discrete```: a discrete space over n integers { 0, 1, ..., n-1 }\n",
    "\n",
    "- ```multi_binary```: a binary space of size n\n",
    "\n",
    "- ```multi_discrete```: allows for multiple discrete spaces with a different number of actions in each\n",
    "\n",
    "- ```tuple```: a tuple space is a tuple of simpler spaces\n",
    "\n",
    "For example, to create a state space $[-1, 2]^{3 \\times 4}$ we do:\n",
    "```Box(low=-1.0, high=2.0, shape=(3, 4), dtype=np.float32)```\n",
    "Unfortunately the last argument, ```dtype```, is annoyingly important.  You will need to make sure to typecast all variables appropriately when interaction with environment.\n",
    "\n",
    "See the documentation [here](https://www.gymlibrary.ml/content/spaces/) to read more about the spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdbed50",
   "metadata": {},
   "source": [
    "## Trade Execution\n",
    "\n",
    "We will be implementing the universal trade order execution environment from [here](https://arxiv.org/abs/2103.10860).  Unfortunately I am not a finance expert so you will have to deal with my imperfect interpretation of the environment, but see the paper for a better explanation.  Suppose you have access to a certain amount $Q$ of a fixed stock.\n",
    "\n",
    "For sake of discussion, lets say [ETD](https://finance.yahoo.com/quote/ETD/) for the Ethan Allen furniture company, a Vermont-based furniture company named after [Ethan Allen](https://en.wikipedia.org/wiki/Ethan_Allen), forming the famous [Green Mountain Boys](https://en.wikipedia.org/wiki/Green_Mountain_Boys) who fought back against Canada, New York, and New Hampshire to take control over the territory now known as Vermont.\n",
    "\n",
    "Unfortunately, the stock is plummeting and so you want to sell your entire volume $Q$ of the stock into the market over a fixed time horizon $T$.  For each round $t = 0, 1, \\ldots, T - 1$ you will observe the price $p_t$ and select a volume $q_{t+1}$ of shares, the trading over will then actually be executed with the next price $p_{t+1}$ due to the information structure in financial markets.  The goal is to maximize the revenue with completed liquidation.  As such, given access to the full vector of prices in the market $p_1, \\ldots, p_{T+1}$ your goal is to pick:\n",
    "\n",
    "$$\n",
    "\\max_{q_1, \\ldots, q_T} \\sum_{t=0}^T q_t p_{t+1} \\text{ s. t. } \\sum_{t=1}^T q_{t} = Q\n",
    "$$\n",
    "\n",
    "The first corresponds to the total revenue, and the second is the constraint for total liquidation.  Note that we can instead enforce that $q_{t} = a_t Q$ where $a_t \\in [0,1]$ to be the action for simpler implementation.  As such, we can then specify the action space and state space via:\n",
    "\n",
    "```\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(1), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1), dtype=np.float32)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbff3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f5bb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = spaces.Box(low=0, high=1, shape=[1], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4132c4",
   "metadata": {},
   "source": [
    "We can sample from the action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e013f9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7483489], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe0183",
   "metadata": {},
   "source": [
    "and test membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2338ec96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space.contains([.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5fbd9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space.contains([1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f898d42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space.contains([1.5,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77251f65",
   "metadata": {},
   "source": [
    "Putting this into the framework from before gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abc2f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class Trading(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, T, Q):\n",
    "        super(Trading, self).__init__()    # Define action and observation space\n",
    "        # They must be gym.spaces objects    # Example when using discrete actions:\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=[1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=[1], dtype=np.float32)\n",
    "        self.T = T\n",
    "        self.Q = Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29974e62",
   "metadata": {},
   "source": [
    "## Reset Function\n",
    "\n",
    "Next we will write the reset method, which is called anytime a new environment is created or to reset an existing environment's state.  This is where we will set the initial information and number of rounds in the experiment, etc.  In our problem, the starting state can be a fixed price of $0.5$ and the time index to be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494d5c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class Trading(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, T, Q):\n",
    "        super(Trading, self).__init__()    # Define action and observation space\n",
    "        # They must be gym.spaces objects    # Example when using discrete actions:\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=[1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=[1], dtype=np.float32)\n",
    "        self.T = T\n",
    "        self.Q = Q\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = [0.5]\n",
    "        self.timestep = 0\n",
    "        self.market_sold = 0 # amount of inventory sold\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe28ea",
   "metadata": {},
   "source": [
    "## Step Function\n",
    "\n",
    "Next our environment needs to be able to take a step.  At each step we will take a specified action (chosen by the algortihm within the action spacE), calculate the reward, return the next observation, and indicate whether or not the experiment is finished running.\n",
    "\n",
    "In this model, we have to do two things:\n",
    "- Calculate the next market price (can just add some Gaussian noise to be simple)\n",
    "- Determine if we are at the last round (where the action does not matter since we need to liquidate entire asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b0bd22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class Trading(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, T, Q):\n",
    "        super(Trading, self).__init__()    # Define action and observation space\n",
    "        # They must be gym.spaces objects    # Example when using discrete actions:\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=[1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=[1], dtype=np.float32)\n",
    "        self.T = T\n",
    "        self.Q = Q\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = [0.5]\n",
    "        self.timestep = 0\n",
    "        self.market_sold = 0 # amount of inventory sold\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        \n",
    "        if self.market_sold + action[0] > 1: # checks if we are selling more than we have\n",
    "            action = 1 - self.market_sold\n",
    "    \n",
    "        done = False\n",
    "        if self.timestep == self.T - 1: # updates action in last timestep\n",
    "            action = 1 - self.market_sold\n",
    "            done = True\n",
    "    \n",
    "        self.market_sold += action\n",
    "        \n",
    "        self.timestep += 1\n",
    "        \n",
    "        # Calculates new price and updates new state\n",
    "        \n",
    "        price = self.state[0]\n",
    "        price = np.clip(price + np.random.uniform(0, .1),0,1)\n",
    "        self.state = [price]\n",
    "        \n",
    "        # Calculates reward\n",
    "        reward = action * self.Q * price\n",
    "        \n",
    "        return self.state, reward, done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a6106",
   "metadata": {},
   "source": [
    "## Render\n",
    "\n",
    "The only thing left is to render.  For simplicity, we will just print out the profit made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc41a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class Trading(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, T, Q):\n",
    "        super(Trading, self).__init__()    # Define action and observation space\n",
    "        # They must be gym.spaces objects    # Example when using discrete actions:\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=[1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=[1], dtype=np.float32)\n",
    "        self.T = T\n",
    "        self.Q = Q\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = [0.5]\n",
    "        self.timestep = 0\n",
    "        self.market_sold = 0 # amount of inventory sold\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        \n",
    "        action = action[0]\n",
    "        if self.market_sold + action > 1: # checks if we are selling more than we have\n",
    "            action = 1 - self.market_sold\n",
    "    \n",
    "        done = False\n",
    "        if self.timestep == self.T - 1: # updates action in last timestep\n",
    "            action = 1 - self.market_sold\n",
    "            done = True\n",
    "    \n",
    "        self.market_sold += action\n",
    "        \n",
    "        self.timestep += 1\n",
    "        \n",
    "        # Calculates new price and updates new state\n",
    "        \n",
    "        price = self.state[0]\n",
    "        price = np.clip(price + np.random.uniform(0, .1),0,1)\n",
    "        self.state = [price]\n",
    "        \n",
    "        # Calculates reward\n",
    "        reward = action * self.Q * price\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        print(f'Current state: {self.state}')\n",
    "        print(f'Shares remaining: {1 - self.market_sold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a46ef",
   "metadata": {},
   "source": [
    "## TaDa!\n",
    "\n",
    "Our environment is now complete.  We can now instantiate an object and test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "664d50cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Trading(5, 100)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09284f9c",
   "metadata": {},
   "source": [
    "Note that calling reset returns the state to be $.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d150541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Trading(5, 100)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afff8cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New state: [0.5731734894850563], reward: 11.463469789701126, done: False\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, _ = env.step([1/5])\n",
    "print(f'New state: {state}, reward: {reward}, done: {done}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4554cc37",
   "metadata": {},
   "source": [
    "Run the step above 5 times to verify that the last component switches over to being finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465dc436",
   "metadata": {},
   "source": [
    "## Incorporating into the package and registering\n",
    "\n",
    "Next up we will incorporate and register the package as part of ORSuite.  When adding an environment we require the following file structure:\n",
    "\n",
    "```\n",
    "or_suite/envs/new_env_name:\n",
    "    -> __init__.py\n",
    "    -> env_name.py\n",
    "    -> env_name_readme.ipynb\n",
    "```\n",
    "\n",
    "The `__init__.py` file should simply import the environment class from `env_name.py`.  \n",
    "Once the environment file structure has been made, in order to include the environment in the package we additionally require:\n",
    "- Specify default parameter values for the configuration dictionary of the environment, which is saved in `or_suite/envs/env_configs.py`\n",
    "- Register the environment by modifying `or_suite/envs/__init__.py` to include a link to the class along with a name\n",
    "- Modify `or_suite/envs/__init__.py` to import the new environment folder.\n",
    "\n",
    "All of this has been done already as an example for the trading environment.  Note that there are a couple modifications that were made just to have consistency in the package including:\n",
    "- Setting up the config as a dictionary instead of passing individual arguments\n",
    "- Adjusting the naming structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631e142",
   "metadata": {},
   "source": [
    "Now that the environment is registered can create one simply by specifying the name of the simulator, here taken to be ```StockTrading-v0```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b322841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = or_suite.envs.env_configs.trade_execution_default_config\n",
    "stock_env = gym.make('StockTrading-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56353cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "742b7a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.53048302]), 13.262075458704226, False, {})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_env.step([.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73852989",
   "metadata": {},
   "source": [
    "We can also verify that the environment is set-up correctly using a handy checker built by the [stablebaselines](https://stable-baselines.readthedocs.io/en/master/) package.  It might be a good idea to do the same for your own environment to double check your code.  Note that doing this for myself also reminded me of some things missed above!\n",
    "\n",
    "- When resetting the returned state needs to be a np array\n",
    "- In the step function the returned state also needs to be a np array\n",
    "\n",
    "(Hence the earlier complaints on the OpenAI Gym framework being picky about datatypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99721044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/anaconda3/envs/custom_simulator/lib/python3.8/site-packages/stable_baselines3/common/env_checker.py:231: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(stock_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cdbbcf",
   "metadata": {},
   "source": [
    "It will give a generic warning about normalizing the action space or rewards, but that is only required for certain DeepRL implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e9993",
   "metadata": {},
   "source": [
    "## Thoughts and comments\n",
    "\n",
    "The OpenAI Gym API framework is great for models where you are investigating approximate dynamic programming in a known MDP, or the online learning setting where the data the algortihm has access to must come from on-policy trajectories.\n",
    "\n",
    "However, another learning framework is the generative model setting where you assume the algorithm is able to query $s', r \\sim r_h(s,a), T_h(\\cdot \\mid s,a)$.  Unfortunately, the step function does not work for this exactly, since it depends on some internal variables that might not be updated or captured when calling the ```step``` function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598e61d",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Your goal for this code demo is to pick ``any`` model, implement it, and incorporate it into the ORSuite package.  Note that if you pick a model with a finite set of states and actions, in the next code demo when we work on developing a value iteration based algorithm we will be able to test it out on the simulator that you implement.  Also make sure to check the environment using the stablebaselines package.\n",
    "\n",
    "For some inspiration:\n",
    "- [Windy Grid World](https://github.com/ibrahim-elshar/gym-windy-gridworlds)\n",
    "- Stochastic Queueing Network\n",
    "- More advanced financial models\n",
    "- [Pandora's Box](https://en.wikipedia.org/wiki/Pandora%27s_box)\n",
    "- [Online Bin Packing](https://en.wikipedia.org/wiki/Bin_packing_problem)\n",
    "- [Scheduling Problems](https://en.wikipedia.org/wiki/Scheduling_(computing))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
